{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Explanation of the Provided Code\n\nThe provided code performs several tasks related to preparing audio, transcribing it, performing speaker diarization, and generating subtitles. Here's a breakdown of each part of the code:\n\n## Downloading Video from YouTube\n\n1. The code starts by installing the `pytube` library for working with YouTube videos.\n\n2. It specifies the YouTube video URL that you want to download and assigns it to the variable `video_url`.\n\n3. A `YouTube` object is created using the provided video URL.\n\n4. The highest resolution stream (video) is obtained using `yt.streams.get_highest_resolution()`.\n\n5. The download path is specified as \"test\".\n\n6. The video is downloaded using `video_stream.download()`.\n\n7. The code then retrieves the path of the downloaded video file.\n\n## Extracting Audio from Video\n\n1. The code installs the `moviepy` library for working with multimedia files.\n\n2. It specifies the output audio file path as \"test.mp3\".\n\n3. The audio is extracted from the downloaded video using `ffmpeg_extract_audio()` and saved as an MP3 file.\n\n4. The path of the saved audio file is printed.\n\n## Transcribing\n\n1. The code sets up the hardware acceleration token for Hugging Face (HF) if you have one. In this case, it is set to a placeholder value.\n\n2. It checks the availability of a CUDA-enabled GPU and assigns the appropriate device (CPU or GPU) to the variable `DEVICE`.\n\n## Generating Script\n\n1. The code installs the `whisper` library, which is used for automatic speech recognition.\n\n2. It specifies the desired ASR (Automatic Speech Recognition) model size (e.g., \"large\").\n\n3. The ASR model is loaded using `whisper.load_model()`.\n\n4. The script (transcription) is generated from the audio file using the loaded ASR model.\n\n## Speaker Diarization\n\n1. The code installs the `whisperX` library, which is used for speaker diarization.\n\n2. It creates a diarization pipeline using the Hugging Face token.\n\n3. Speaker diarization is performed on the audio using the diarization pipeline.\n\n## Combining Script with Speaker Diarization\n\n1. The code aligns the generated script with the speaker diarization results.\n\n2. It loads an align model and metadata using `whisperx.load_align_model()`.\n\n3. The script segments are aligned with the audio using `whisperx.align()`.\n\n4. Speaker information is assigned to word segments, creating a list of transcribed segments with speaker labels.\n\n## Generating Subtitles File\n\n1. The code specifies the output SubRip (`.srt`) subtitles file path as \"subtitles.srt\".\n\n2. It opens the `.srt` file for writing.\n\n3. The code iterates through the transcribed segments, converts the start and end times to the SubRip format, and writes each subtitle entry to the `.srt` file.\n\n4. Speaker names are mapped from codes (e.g., \"SPEAKER_00\") to actual names (e.g., \"Ali\").\n\n5. The `.srt` file is created with subtitle entries following the SubRip format.\n\n6. A message is printed to indicate the successful creation of the `.srt` file.\n\nThis code is designed to download a video from YouTube, extract its audio, transcribe the audio, perform speaker diarization, and generate subtitles. It leverages various libraries and Hugging Face models to automate these tasks.","metadata":{}},{"cell_type":"markdown","source":"# Original Video","metadata":{}},{"cell_type":"code","source":"from IPython.display import HTML\nHTML('<div align=\"center\"><iframe align = \"middle\" width=\"790\" height=\"440\" src=\"https://www.youtube.com/embed/qR4JwjI3ldU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:48:01.861517Z","iopub.execute_input":"2023-09-17T11:48:01.861935Z","iopub.status.idle":"2023-09-17T11:48:01.869971Z","shell.execute_reply.started":"2023-09-17T11:48:01.861902Z","shell.execute_reply":"2023-09-17T11:48:01.868963Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div align=\"center\"><iframe align = \"middle\" width=\"790\" height=\"440\" src=\"https://www.youtube.com/embed/qR4JwjI3ldU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Transcribed Video","metadata":{}},{"cell_type":"code","source":"from IPython.display import HTML\nHTML('<div align=\"center\"><iframe align = \"middle\" width=\"790\" height=\"440\" src=\"https://www.youtube.com/embed/4jQzoXxlzMU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T12:07:46.771001Z","iopub.execute_input":"2023-09-17T12:07:46.772204Z","iopub.status.idle":"2023-09-17T12:07:46.781123Z","shell.execute_reply.started":"2023-09-17T12:07:46.772148Z","shell.execute_reply":"2023-09-17T12:07:46.779659Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div align=\"center\"><iframe align = \"middle\" width=\"790\" height=\"440\" src=\"https://www.youtube.com/embed/4jQzoXxlzMU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preparing the Audio","metadata":{"id":"A1PVhRfadjZi"}},{"cell_type":"markdown","source":"## Downloading Video from Youtube","metadata":{"id":"ntBxhPnbd1VN"}},{"cell_type":"code","source":"!pip install pytube","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gckQH07xdnE5","outputId":"de9e09e1-5c29-409c-d5ce-cff3973cdca8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Collecting pytube\n\n  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hInstalling collected packages: pytube\n\nSuccessfully installed pytube-15.0.0\n"}]},{"cell_type":"code","source":"from pytube import YouTube\n\n# Input the YouTube video URL\nvideo_url = \"https://www.youtube.com/watch?v=qR4JwjI3ldU\"\n\n# Create a YouTube object\nyt = YouTube(video_url)\n\n# Get the highest resolution stream (usually it's the first stream in the list)\nvideo_stream = yt.streams.get_highest_resolution()\n\n# Provide the download path where you want to save the video\ndownload_path = \"test\"\n\n# Download the video\nvideo_stream.download(output_path=download_path)\n\n# getting the path\nimport os\nvideo_path = os.path.join(download_path, os.listdir(download_path)[0])\nprint(\"video downloaded at:\", video_path)","metadata":{"id":"aPyJtvGXJkmH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a9146f5-8a12-4f32-8373-43a32f96b8a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"video downloaded at: test/Muhammad Ali Speech - Value Of Education.mp4\n"}]},{"cell_type":"markdown","source":"## Extracting Audio from Video","metadata":{"id":"UmtTky-SeKVY"}},{"cell_type":"code","source":"! pip install moviepy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzdNajopc3S_","outputId":"9055acfb-c91f-469a-86e6-d75d3af076dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n\nRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n\nRequirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n\nRequirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n\nRequirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.3)\n\nRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n\nRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.2.0)\n\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.7.22)\n"}]},{"cell_type":"code","source":"from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_audio\n\n# Output audio file path (MP3)\naudio_path = \"test.mp3\"\n\n# Extract audio from the video and save it as MP3\nffmpeg_extract_audio(video_path, audio_path)\nprint(\"audio saved at:\", audio_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1Z59sOsc3Vg","outputId":"7cf051c7-fdb1-486d-b583-1de042cd1fe2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Moviepy - Running:\n\n>>> \"+ \" \".join(cmd)\n\nMoviepy - Command successful\n\naudio saved at: test.mp3\n"}]},{"cell_type":"markdown","source":"# Transcribing","metadata":{"id":"umuQLsjDemob"}},{"cell_type":"code","source":"# https://huggingface.co/settings/tokens\nHF_TOKEN = \"here goes your hugging face code\"","metadata":{"id":"09jmm7RZfrDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTZm2xgiezHq","outputId":"b9c32466-013b-4915-a104-538a19a74c1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":["device(type='cuda')"]},"metadata":{}}]},{"cell_type":"markdown","source":"## Generating Script","metadata":{"id":"SB9XHohIeptQ"}},{"cell_type":"code","source":"! pip install git+https://github.com/openai/whisper.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ld8tKq2eu1Y","outputId":"0229c377-91fe-48cd-b5d9-d392513153aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Collecting git+https://github.com/openai/whisper.git\n\n  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-9jxvu_dm\n\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-9jxvu_dm\n\n  Resolved https://github.com/openai/whisper.git to commit e8622f9afc4eba139bf796c210f5c01081000472\n\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.23.5)\n\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.1+cu118)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.66.1)\n\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (10.1.0)\n\nCollecting tiktoken==0.3.3 (from openai-whisper==20230314)\n\n  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2023.6.3)\n\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.31.0)\n\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.27.4.1)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.2)\n\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.6)\n\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.12)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.2.0)\n\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.4)\n\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2023.7.22)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.3)\n\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n\nBuilding wheels for collected packages: openai-whisper\n\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\n  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798395 sha256=01fb627994711fed80d725a107d59ea9c1161a8a5779d3ec84a6468e99e46760\n\n  Stored in directory: /tmp/pip-ephem-wheel-cache-u695hmqk/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n\nSuccessfully built openai-whisper\n\nInstalling collected packages: tiktoken, openai-whisper\n\nSuccessfully installed openai-whisper-20230314 tiktoken-0.3.3\n"}]},{"cell_type":"code","source":"import whisper\n\nmodel_name = \"large\"   # tiny | base | small | medium  | large\nmodel = whisper.load_model(model_name, DEVICE)\nscript = model.transcribe(audio_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFmCgrKMesEa","outputId":"da999a35-da67-4ca6-b394-c926e4c2311d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"100%|█████████████████████████████████████| 2.87G/2.87G [00:33<00:00, 91.7MiB/s]\n"}]},{"cell_type":"code","source":"script['text'][:1000]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"TMzePweTxbbz","outputId":"37feef0a-18c8-4ac5-f88f-57440b8c527a"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":["\" Sir, you there. Are you going to teach your son to be a fighter? Bring him up to be a fighter? No, sir. My son is already, he's two years old and we're starting, we want him to learn three languages. Arabic, French, and Spanish. My son is going to, by the time he's where he is, we in America would have had what we call Independence for now. The separation we preach would have been taking place in maybe another, less than ten years. This is going to happen. God's going to force it. By then, he's going to be, I hope to be a world traveler, an interpreter, talking to other people. In French, most, many African people speak only French. Many darker people speak Spanish. And he's going to have to do a lot of traveling, ambassador work, and doing different things, I plan. And I'm making, he's learning these three, he's two years old, and we're getting him ready now for Arabic, French, and Spanish. And so he ain't going to be no fighter, he's going to use his brain. Why not? Why wouldn't you\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Speaker Diarization","metadata":{"id":"fXY8UajifOAW"}},{"cell_type":"code","source":"! pip install git+https://github.com/m-bain/whisperX.git","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"Y5cop_Fffd8v","outputId":"f305b7c3-ae86-406d-9f25-031d79adff89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Installing collected packages: tokenizers, sentencepiece, safetensors, python-editor, primePy, docopt, av, antlr4-python3-runtime, websockets, tensorboardX, shellingham, semver, ruamel.yaml.clib, readchar, python-multipart, ordered-set, omegaconf, Mako, lightning-utilities, humanfriendly, h11, ffmpeg-python, einops, ctranslate2, colorlog, colorama, cmaes, blessed, backoff, uvicorn, starlette, ruamel.yaml, pyannote.core, inquirer, huggingface-hub, deepdiff, dateutils, croniter, coloredlogs, arrow, alembic, transformers, starsessions, optuna, onnxruntime, hyperpyyaml, fastapi, pyannote.database, lightning-cloud, faster-whisper, pyannote.pipeline, pyannote.metrics, torchmetrics, torch-pitch-shift, pytorch-lightning, julius, torch_audiomentations, speechbrain, pytorch_metric_learning, lightning, asteroid-filterbanks, pyannote.audio, whisperx\n\nSuccessfully installed Mako-1.2.4 alembic-1.12.0 antlr4-python3-runtime-4.9.3 arrow-1.2.3 asteroid-filterbanks-0.4.0 av-10.0.0 backoff-2.2.1 blessed-1.20.0 cmaes-0.10.0 colorama-0.4.6 coloredlogs-15.0.1 colorlog-6.7.0 croniter-1.4.1 ctranslate2-3.19.0 dateutils-0.6.12 deepdiff-6.5.0 docopt-0.6.2 einops-0.6.1 fastapi-0.103.1 faster-whisper-0.8.0 ffmpeg-python-0.2.0 h11-0.14.0 huggingface-hub-0.17.1 humanfriendly-10.0 hyperpyyaml-1.2.1 inquirer-3.1.3 julius-0.2.7 lightning-2.0.9 lightning-cloud-0.5.38 lightning-utilities-0.9.0 omegaconf-2.3.0 onnxruntime-1.15.1 optuna-3.3.0 ordered-set-4.1.0 primePy-1.3 pyannote.audio-2.1.1 pyannote.core-5.0.0 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pyannote.pipeline-2.3 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.9 pytorch_metric_learning-2.3.0 readchar-4.0.5 ruamel.yaml-0.17.28 ruamel.yaml.clib-0.2.7 safetensors-0.3.3 semver-3.0.1 sentencepiece-0.1.99 shellingham-1.5.3 speechbrain-0.5.15 starlette-0.27.0 starsessions-1.3.0 tensorboardX-2.6.2.2 tokenizers-0.13.3 torch-pitch-shift-1.2.4 torch_audiomentations-0.11.0 torchmetrics-1.1.2 transformers-4.33.2 uvicorn-0.23.2 websockets-11.0.3 whisperx-3.1.1\n"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","source":"from whisperx.diarize import DiarizationPipeline\n\ndiarization_pipeline = DiarizationPipeline(use_auth_token=HF_TOKEN)\ndiarized = diarization_pipeline(audio_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IHzqlMlc3Zo","outputId":"ad533732-d82c-4104-b3d3-308adcac131e"},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.9. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"},{"name":"stdout","output_type":"stream","text":"Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"}]},{"cell_type":"code","source":"diarized","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":770},"id":"P8upk0C8xXTI","outputId":"590b14a0-f3ec-4c07-ad6c-52b5b3598418"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":["                                    0  1     speaker       start         end\n","0   [ 00:00:00.497 -->  00:00:01.763]  P  SPEAKER_01    0.497812    1.763437\n","1   [ 00:00:02.404 -->  00:00:06.775]  Q  SPEAKER_01    2.404688    6.775313\n","2   [ 00:00:05.897 -->  00:00:06.741]  A  SPEAKER_00    5.897813    6.741563\n","3   [ 00:00:07.737 -->  00:00:09.255]  B  SPEAKER_00    7.737188    9.255938\n","4   [ 00:00:09.998 -->  00:00:14.132]  C  SPEAKER_00    9.998438   14.132813\n","5   [ 00:00:14.824 -->  00:00:15.432]  D  SPEAKER_00   14.824688   15.432188\n","6   [ 00:00:16.512 -->  00:00:18.419]  E  SPEAKER_00   16.512187   18.419062\n","7   [ 00:00:19.701 -->  00:00:32.442]  F  SPEAKER_00   19.701563   32.442188\n","8   [ 00:00:33.184 -->  00:00:55.628]  G  SPEAKER_00   33.184688   55.628438\n","9   [ 00:00:56.320 -->  00:01:00.758]  H  SPEAKER_00   56.320313   60.758438\n","10  [ 00:01:00.117 -->  00:01:02.378]  R  SPEAKER_01   60.117188   62.378438\n","11  [ 00:01:02.648 -->  00:01:03.407]  I  SPEAKER_00   62.648438   63.407813\n","12  [ 00:01:03.745 -->  00:01:06.816]  S  SPEAKER_01   63.745313   66.816563\n","13  [ 00:01:05.787 -->  00:01:07.238]  J  SPEAKER_00   65.787188   67.238438\n","14  [ 00:01:07.761 -->  00:01:09.584]  T  SPEAKER_01   67.761562   69.584063\n","15  [ 00:01:09.044 -->  00:01:17.819]  K  SPEAKER_00   69.044062   77.819063\n","16  [ 00:01:17.987 -->  00:01:23.505]  U  SPEAKER_01   77.987813   83.505938\n","17  [ 00:01:23.505 -->  00:01:33.614]  L  SPEAKER_00   83.505938   93.614063\n","18  [ 00:01:34.592 -->  00:02:10.165]  M  SPEAKER_00   94.592813  130.165313\n","19  [ 00:02:09.439 -->  00:02:17.067]  V  SPEAKER_01  129.439687  137.067187\n","20  [ 00:02:17.033 -->  00:04:05.421]  N  SPEAKER_00  137.033437  245.421562\n","21  [ 00:04:06.248 -->  00:04:41.837]  O  SPEAKER_00  246.248438  281.837813\n","22  [ 00:04:41.837 -->  00:04:42.951]  W  SPEAKER_01  281.837813  282.951563"],"text/html":["\n","  <div id=\"df-6050b7b5-5483-4c5c-a362-69fcea5864ac\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>speaker</th>\n","      <th>start</th>\n","      <th>end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[ 00:00:00.497 --&gt;  00:00:01.763]</td>\n","      <td>P</td>\n","      <td>SPEAKER_01</td>\n","      <td>0.497812</td>\n","      <td>1.763437</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[ 00:00:02.404 --&gt;  00:00:06.775]</td>\n","      <td>Q</td>\n","      <td>SPEAKER_01</td>\n","      <td>2.404688</td>\n","      <td>6.775313</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ 00:00:05.897 --&gt;  00:00:06.741]</td>\n","      <td>A</td>\n","      <td>SPEAKER_00</td>\n","      <td>5.897813</td>\n","      <td>6.741563</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[ 00:00:07.737 --&gt;  00:00:09.255]</td>\n","      <td>B</td>\n","      <td>SPEAKER_00</td>\n","      <td>7.737188</td>\n","      <td>9.255938</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[ 00:00:09.998 --&gt;  00:00:14.132]</td>\n","      <td>C</td>\n","      <td>SPEAKER_00</td>\n","      <td>9.998438</td>\n","      <td>14.132813</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[ 00:00:14.824 --&gt;  00:00:15.432]</td>\n","      <td>D</td>\n","      <td>SPEAKER_00</td>\n","      <td>14.824688</td>\n","      <td>15.432188</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[ 00:00:16.512 --&gt;  00:00:18.419]</td>\n","      <td>E</td>\n","      <td>SPEAKER_00</td>\n","      <td>16.512187</td>\n","      <td>18.419062</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[ 00:00:19.701 --&gt;  00:00:32.442]</td>\n","      <td>F</td>\n","      <td>SPEAKER_00</td>\n","      <td>19.701563</td>\n","      <td>32.442188</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>[ 00:00:33.184 --&gt;  00:00:55.628]</td>\n","      <td>G</td>\n","      <td>SPEAKER_00</td>\n","      <td>33.184688</td>\n","      <td>55.628438</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>[ 00:00:56.320 --&gt;  00:01:00.758]</td>\n","      <td>H</td>\n","      <td>SPEAKER_00</td>\n","      <td>56.320313</td>\n","      <td>60.758438</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>[ 00:01:00.117 --&gt;  00:01:02.378]</td>\n","      <td>R</td>\n","      <td>SPEAKER_01</td>\n","      <td>60.117188</td>\n","      <td>62.378438</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>[ 00:01:02.648 --&gt;  00:01:03.407]</td>\n","      <td>I</td>\n","      <td>SPEAKER_00</td>\n","      <td>62.648438</td>\n","      <td>63.407813</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>[ 00:01:03.745 --&gt;  00:01:06.816]</td>\n","      <td>S</td>\n","      <td>SPEAKER_01</td>\n","      <td>63.745313</td>\n","      <td>66.816563</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>[ 00:01:05.787 --&gt;  00:01:07.238]</td>\n","      <td>J</td>\n","      <td>SPEAKER_00</td>\n","      <td>65.787188</td>\n","      <td>67.238438</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>[ 00:01:07.761 --&gt;  00:01:09.584]</td>\n","      <td>T</td>\n","      <td>SPEAKER_01</td>\n","      <td>67.761562</td>\n","      <td>69.584063</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>[ 00:01:09.044 --&gt;  00:01:17.819]</td>\n","      <td>K</td>\n","      <td>SPEAKER_00</td>\n","      <td>69.044062</td>\n","      <td>77.819063</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[ 00:01:17.987 --&gt;  00:01:23.505]</td>\n","      <td>U</td>\n","      <td>SPEAKER_01</td>\n","      <td>77.987813</td>\n","      <td>83.505938</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>[ 00:01:23.505 --&gt;  00:01:33.614]</td>\n","      <td>L</td>\n","      <td>SPEAKER_00</td>\n","      <td>83.505938</td>\n","      <td>93.614063</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>[ 00:01:34.592 --&gt;  00:02:10.165]</td>\n","      <td>M</td>\n","      <td>SPEAKER_00</td>\n","      <td>94.592813</td>\n","      <td>130.165313</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>[ 00:02:09.439 --&gt;  00:02:17.067]</td>\n","      <td>V</td>\n","      <td>SPEAKER_01</td>\n","      <td>129.439687</td>\n","      <td>137.067187</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>[ 00:02:17.033 --&gt;  00:04:05.421]</td>\n","      <td>N</td>\n","      <td>SPEAKER_00</td>\n","      <td>137.033437</td>\n","      <td>245.421562</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>[ 00:04:06.248 --&gt;  00:04:41.837]</td>\n","      <td>O</td>\n","      <td>SPEAKER_00</td>\n","      <td>246.248438</td>\n","      <td>281.837813</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>[ 00:04:41.837 --&gt;  00:04:42.951]</td>\n","      <td>W</td>\n","      <td>SPEAKER_01</td>\n","      <td>281.837813</td>\n","      <td>282.951563</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6050b7b5-5483-4c5c-a362-69fcea5864ac')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6050b7b5-5483-4c5c-a362-69fcea5864ac button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6050b7b5-5483-4c5c-a362-69fcea5864ac');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1fabc247-9468-4b40-8ded-13b91b124b23\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1fabc247-9468-4b40-8ded-13b91b124b23')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1fabc247-9468-4b40-8ded-13b91b124b23 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":"## Combining Script with Speaker Diarization","metadata":{"id":"E5AwUYxSgYf-"}},{"cell_type":"code","source":"from whisperx import load_align_model, align\nfrom whisperx.diarize import assign_word_speakers","metadata":{"id":"aWtQ-BxehqdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Align Script\nmodel_a, metadata = load_align_model(language_code=script[\"language\"], device=DEVICE)\nscript_aligned = align(script[\"segments\"], model_a, metadata, audio_path, DEVICE)\n\n# Align Speakers\nresult_segments, word_seg = list(assign_word_speakers(\n    diarized, script_aligned\n).values())\ntranscribed = []\nfor result_segment in result_segments:\n    transcribed.append(\n        {\n            \"start\": result_segment[\"start\"],\n            \"end\": result_segment[\"end\"],\n            \"text\": result_segment[\"text\"],\n            \"speaker\": result_segment[\"speaker\"],\n        }\n    )","metadata":{"id":"OJRUGlDmgbmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for start, end, text, speaker in [i.values() for i in transcribed]:\n    print(start, end, speaker, text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4MvHE7ViHPJ","outputId":"9caedeca-b8e2-4d4e-cea7-f871d4f59663"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"0.522 2.609 SPEAKER_01  Sir, you there.\n\n2.609 5.338 SPEAKER_01 Are you going to teach your son to be a fighter?\n\n5.338 5.98 SPEAKER_01 Bring him up to be a fighter?\n\n6.02 7.845 SPEAKER_00  No, sir.\n\n7.845 13.88 SPEAKER_00 My son is already, he's two years old and we're starting, we want him to learn three languages.\n\n15.042 19.831 SPEAKER_00  Arabic, French, and Spanish.\n\n19.831 25.0 SPEAKER_00 My son is going to, by the time he's where he is, we in America would have had what we call\n\n25.642 27.027 SPEAKER_00  Independence for now.\n\n27.027 30.98 SPEAKER_00 The separation we preach would have been taking place in maybe another, less than ten years.\n\n31.04 33.268 SPEAKER_00  This is going to happen.\n\n33.268 34.532 SPEAKER_00 God's going to force it.\n\n34.532 37.0 SPEAKER_00 By then, he's going to be, I hope to be a world traveler,\n\n37.542 39.829 SPEAKER_00  an interpreter, talking to other people.\n\n39.829 43.0 SPEAKER_00 In French, most, many African people speak only French.\n\n44.024 46.173 SPEAKER_00  Many darker people speak Spanish.\n\n46.173 48.0 SPEAKER_00 And he's going to have to do a lot of traveling, ambassador work,\n\n48.924 50.771 SPEAKER_00  and doing different things, I plan.\n\n50.771 52.96 SPEAKER_00 And I'm making, he's learning these three, he's two years old,\n\n53.722 57.555 SPEAKER_00  and we're getting him ready now for Arabic, French, and Spanish.\n\n57.555 58.98 SPEAKER_00 And so he ain't going to be no fighter,\n\n59.101 60.45 SPEAKER_00  he's going to use his brain.\n\n60.45 61.114 SPEAKER_01 Why not?\n\n61.114 62.0 SPEAKER_01 Why wouldn't you let him be a fighter?\n\n63.025 63.809 SPEAKER_00  For what?\n\n63.809 65.256 SPEAKER_01 Well, I mean, you're a fighter.\n\n65.256 66.0 SPEAKER_01 I mean, why wouldn't you let him?\n\n66.282 67.913 SPEAKER_00  I had to be a fighter.\n\n67.913 68.919 SPEAKER_01 You would have been something else.\n\n69.221 74.0 SPEAKER_00  If I could have went to school and got an education and could speak, say, three or four languages fluently,\n\n74.964 78.036 SPEAKER_00  and got a good trade in life, I'd have forgot all about fighting.\n\n78.036 78.759 SPEAKER_01 Would you?\n\n78.759 78.98 SPEAKER_01 Yeah.\n\n79.946 82.0 SPEAKER_01  When you were at school, I mean, did you miss out on education?\n\n82.081 83.007 SPEAKER_01  No, I didn't miss out.\n\n83.007 84.034 SPEAKER_00 I wouldn't learn.\n\n84.034 85.0 SPEAKER_00 I didn't know the value of it.\n\n85.06 87.577 SPEAKER_00  I was a little black boy in Kentucky, didn't know nothing about it.\n\n87.577 88.0 SPEAKER_00 I'd be on your show,\n\n88.101 89.548 SPEAKER_00  didn't know nothing about it.\n\n89.548 92.0 SPEAKER_00 I'd be a world champion, playing hooky, playing with the girls,\n\n92.06 98.0 SPEAKER_00  not just going to recess and eating lunch and joking, not knowing the value of education,\n\n98.623 102.0 SPEAKER_00  like many children do in the day, never thinking that one day I'd have to feed myself,\n\n102.402 107.0 SPEAKER_00  didn't have no idea of profit and getting out and making money, like a lot of children do.\n\n107.783 113.0 SPEAKER_00  And time went by and I graduated and I won the Olympics and they put me out just because I could fight.\n\n113.503 115.074 SPEAKER_00  And my mind was mainly on sports.\n\n115.074 116.0 SPEAKER_00 They just let me go.\n\n116.02 119.0 SPEAKER_00  Now all of a sudden I find myself having trouble reading things, looking at contracts,\n\n119.0 121.054 SPEAKER_00  got to hire lawyers, to watch lawyers.\n\n121.054 122.0 SPEAKER_00 I know the value of education now.\n\n122.0 124.794 SPEAKER_00  So I'm going to brainwash my children now.\n\n124.794 125.98 SPEAKER_00 My daughter's six years old.\n\n126.261 129.136 SPEAKER_00  She speaks Arabic, English, and Spanish.\n\n129.136 129.759 SPEAKER_00 She's six.\n\n130.662 135.98 SPEAKER_01  In fact, when you left school, Mohammed, were you, in fact, semi-literate in the sense that you're difficult to read?\n\n136.541 138.787 SPEAKER_00  No, not that type of literate.\n\n138.787 144.0 SPEAKER_00 I have a wisdom that can make me talk to you, an educated man on any subject,\n\n144.503 146.94 SPEAKER_00  and if the audience or the people listen, they'll say I won.\n\n147.842 149.346 SPEAKER_00  Now, I'll say this.\n\n149.346 150.469 SPEAKER_00 No, I'm serious.\n\n150.469 155.0 SPEAKER_00 I have the type of knowledge that Jesus had, Moses had, Abraham, Elijah, Mohammed.\n\n155.523 158.0 SPEAKER_00  Men are illiterate according to your educational standard.\n\n159.045 160.995 SPEAKER_00  Moses was so illiterate he couldn't talk.\n\n160.995 162.0 SPEAKER_00 His brother Aaron talked for him.\n\n162.964 165.233 SPEAKER_00  Jesus was a carpenter, know a lot.\n\n165.233 167.0 SPEAKER_00 All of God's prophets were uneducated men.\n\n167.422 172.0 SPEAKER_00  This is why he chooses them, because he wants nobody to take the credit for his success.\n\n172.582 173.245 SPEAKER_00  He's God.\n\n173.245 175.373 SPEAKER_00 He takes an empty and teaches.\n\n175.373 177.0 SPEAKER_00 So I've been taught by Elijah and Mohammed.\n\n177.764 178.668 SPEAKER_00  I studied life.\n\n178.668 181.0 SPEAKER_00 I studied people, and I'm educated on this.\n\n181.563 183.915 SPEAKER_00  But when it comes to reading and writing, I'm not.\n\n183.915 185.0 SPEAKER_00 I may be illiterate in that.\n\n185.441 190.94 SPEAKER_00  But when it comes to common sense, when it comes to feelings, when it comes to love, compassion, heart for people,\n\n191.622 193.952 SPEAKER_00  then I'm rich.\n\n193.952 196.0 SPEAKER_00 I wrote something once that says, where is man's wealth?\n\n196.884 198.771 SPEAKER_00  His wealth is in his knowledge.\n\n198.771 200.96 SPEAKER_00 For if his wealth was in the bank and not in his knowledge,\n\n201.08 205.334 SPEAKER_00  then he don't possess it because it's in the bank.\n\n205.334 206.799 SPEAKER_00 My wealth is in my knowledge.\n\n207.361 209.851 SPEAKER_00  See, I'm a boxer, and I challenge.\n\n209.851 212.0 SPEAKER_00 I love to come on your show because you ask me good questions.\n\n212.221 214.754 SPEAKER_00  I really like a man like you.\n\n214.754 216.0 SPEAKER_00 You asked me a question here that was good.\n\n216.241 218.111 SPEAKER_00  You didn't write that.\n\n218.111 219.296 SPEAKER_00 It was already in the book.\n\n219.296 219.98 SPEAKER_00 You just brought it out.\n\n220.503 221.829 SPEAKER_00  You found that.\n\n221.829 224.0 SPEAKER_00 See, ain't nobody else wise enough to go find that.\n\n224.502 226.067 SPEAKER_00  See, I like people like you.\n\n226.067 226.809 SPEAKER_00 You make me think.\n\n226.809 229.137 SPEAKER_00 You keep me sharp because I'm a spokesman for my people.\n\n230.081 231.53 SPEAKER_00  And I want to represent my people.\n\n231.53 232.98 SPEAKER_00 And I don't want to be a bad representative.\n\n233.282 235.416 SPEAKER_00  I can't be blind because the blind lead the blind.\n\n235.416 236.0 SPEAKER_00 They all fall in the ditch.\n\n236.0 238.492 SPEAKER_00  So I need people like you to match with.\n\n238.492 239.96 SPEAKER_00 George Foreman can't talk to you now.\n\n240.281 241.95 SPEAKER_00  Joe Frazier can't talk to you.\n\n241.95 243.216 SPEAKER_00 You'll eat him up.\n\n243.216 243.859 SPEAKER_00 How you feel, Joe?\n\n245.141 247.932 SPEAKER_00  I've been around six miles every day.\n\n247.932 249.719 SPEAKER_00 I've been left there like I do here.\n\n250.522 251.324 SPEAKER_00  I'm not like that.\n\n251.324 252.328 SPEAKER_00 I don't want to be like that.\n\n252.328 256.0 SPEAKER_00 I want to be intelligent, nice looking, get my briefcase and fix up,\n\n256.04 261.94 SPEAKER_00  and dress up, and go to my boat town, and go give a nice lecture at Harvard University on the intoxications of life.\n\n262.542 267.759 SPEAKER_00  I have a lecture entitled The Purpose of Life, or The Art of Personality, or The Journey to the Golden Life,\n\n268.221 270.669 SPEAKER_00  or The Heart of Man, all type of lectures.\n\n270.669 273.94 SPEAKER_00 The solution to the black and white conflict, religious draft.\n\n274.04 275.424 SPEAKER_00  You understand?\n\n275.424 276.447 SPEAKER_00 I like to study.\n\n276.447 280.98 SPEAKER_00 And like what you're hearing now, things people never heard, I could go on and on and on on different subjects.\n\n281.0 281.898 SPEAKER_00  You understand?\n"}]},{"cell_type":"markdown","source":"# Generating Subtitles File","metadata":{"id":"85tgNFT6iJNE"}},{"cell_type":"code","source":"# Output .srt file path\nsrt_file_path = \"subtitles.srt\"\n\n# Open the .srt file for writing\nwith open(srt_file_path, 'w') as srt_file:\n    count = 1  # Initialize subtitle count\n\n    for entry in transcribed:\n        start_time = entry[\"start\"]\n        end_time = entry[\"end\"]\n\n        speaker = entry[\"speaker\"]\n        speaker = {\n            \"SPEAKER_00\": \"Ali\",\n            \"SPEAKER_01\": \"Host\",\n        }[speaker]\n\n        text = speaker + \": \" + entry[\"text\"]\n\n        # Convert times to the SubRip format (hours:minutes:seconds,milliseconds)\n        start_time_srt = '{:02}:{:02}:{:06.3f}'.format(int(start_time // 3600), int((start_time % 3600) // 60), start_time % 60)\n        end_time_srt = '{:02}:{:02}:{:06.3f}'.format(int(end_time // 3600), int((end_time % 3600) // 60), end_time % 60)\n\n        # Write the subtitle entry to the .srt file\n        srt_file.write(str(count) + '\\n')\n        srt_file.write(start_time_srt + ' --> ' + end_time_srt + '\\n')\n        srt_file.write(text + '\\n\\n')\n\n        count += 1  # Increment subtitle count\n\nprint(f\".srt file '{srt_file_path}' created successfully.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTHPlwB82uIX","outputId":"2607b718-674d-4d5c-a9a2-d23135cd3a96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":".srt file 'subtitles.srt' created successfully.\n"}]}]}